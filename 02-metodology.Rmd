# Metodologia badawcza

```{=latex}
\fancyhead[LO]{\textbf{\small{Metodologia badawcza}}}
\renewcommand{\headrulewidth}{0.5pt}
```

## Wybór danych

Za przedmiot badań obrane zostały notowania pięciu polskich spółek notowanych na Giełdzie Papierów Wartościowych.
Spółki te zostały wybrane tak, aby reprezentowały różne sektory rynku.
Poddano badaniu następujące firmy:

1. **PKN Orlen** -- Polski Koncern Naftowy Orlen Spółka Akcyjna -- polska spółka z ponad sześćdziesięcioletnią historią w **branży paliwowej**.
Opisywana jako koncern wielobranżowy, opiera swoją działalność na poszukiwaniu, wydobyciu, logistyce oraz dystrybucji i sprzedaży różnego rodzaju paliw, m.in. ropy naftowej, gazu ziemnego oraz biopaliw.
Ponadto spółka udziela się na rynkach energii elektrycznej i cieplnej, produkcji nawozów sztucznych, petrochemii, a także na rynku prasowym [@orlenWiki].
Pod szyldem _Orlen_ działa 7 rafinerii w Polsce, Czechach oraz na Litwie, które przerabiają rocznie ponad 30 mln ton ropy naftowej.
Spółka posiada ponad 3000 stacji benzynowych w Europie i sprzedaje 40 produktów petrochemicznych [@orlenSelf].

2. **PKO BP** -- Powszechna Kasa Oszczędności Bank Polski Spółka Akcyjna -- największy bank uniwersalny w Polsce, reprezentujący **branżę finansową**.
Utworzona 7.02.2019 r. jako _Pocztowa Kasa Oszczędności_, która przyczyniła się m.in. do wprowadzenia złotego polskiego do obiegu w Polsce.
Po 1948 r. zlikwidowana i powołana ponownie, tym razem jako bank Państwowy (już pod obecną nazwą), a z dniem 1.07.1975 r. włączona w struktury _Narodowego Banku Polskiego_.
Jako _bank uniwersalny_ obsługuje różne podmioty, od osób fizycznych, poprzez małe i średnie przedsiębiorstwa, skończywszy na wielkich korporacjach [@pkoWiki].
Misją banku jest pozytywny wpływ na Polskę, tj. ludzi, firmy, kulturę i środowisko. Określany jako jeden z największych banków w Europie Środkowej i Wschodniej.
W posiadaniu spółki jest również kolekcja różnego rodzaju dzieł sztuki, a pod jej szyldem funkcjonuje również _Fundacja_, realizująca m.in. projekty lokalne i indywidualne [@pkoSelf].

3. **11 bit studios** -- polskie przedsiębiorstwo założone w 2009 roku z siedzibą w Warszawie, zajmującą się produkcją i wydawaniem **gier komputerowych**.
Założone w większości przez byłych pracowników _Metropolis Software_, innego polskiego producenta gier, który w wyniku braku sukcesów na rynku zmuszony był zakończyć swoją działalność.
Nowo utworzone studio wydało kilka produkcji, z czego szczególnym sukcesem cieszy się gra strategiczna z gatunku _city builder_: _Frostpunk_ [@11bitWiki].
Będąc laureatem wielu wyróżnień, m.in. na najlepszą grę strategiczną podczas _Video Game Awards_ oraz najlepszą grę polską od _Digital Dragons_, studio stale rozszerza swój dorobek, pracując obecnie nad kontynuacją swojej hitowej produkcji.
Produkcję gier traktują na równi z twórczością artystyczną, chcąc konstruować "uniwersa pełne prowokujących do myślenia powieści" [@11bitSelf].

4. **Develia** -- spółka akcyjna kontrolująca grupę spółek zajmujących się nabywaniem gruntów i budowaniem, sprzedażą oraz najmem **nieruchomości** o przeznaczeniu mieszkaniowym, biurowym, handlowym i usługowym.
Spółka giełdowa, wcześniej znana jako _LC Corp S.A._, notowana od 2007 r. na Warszawskiej Giełdzie Papierów Wartościowych.
Koncentruje się głównie na projektach mieszkaniowych w największych miastach Polski [@develiaWiki].
Jako czołowy polski deweloper projektuje osiedla z myślą o zaspokojeniu potrzeb mieszkańców i jednoczesnym zachowaniu minimalnego wpływu na ekosystem.
W ciągu 16 lat działalności firma oddała do użytku ponad 17 tys. lokali, a przykładem jej projektów w Poznaniu jest _Malta Point_ przy ul. Brneńskiej [@develiaSelf].

5. **Synektik** -- polska spółka akcyjna dostarczająca wiodące na rynku rozwiązania informatyczne oraz radiofarmaceutyczne, zapewniając wsparcie **medyczne** wielu placówkom.
Firma zajmuje się m.in. sprzedażą sprzętu medycznego do diagnostyki i terapii, usługi w zakresie serwisowania sprzętu medycznego, a także produkcją radiofarmaceutyków.
Spółka utworzona została 4.10.2001 r. jako spółka z ograniczoną odpowiedzialnością i początkowo wykonywała montaże i instalacje oraz serwis aparatury specjalistycznej.
Po przekształceniu w spółkę akcyjną w 2011 r. firma otrzymała zezwolenie na wytwarzanie radiofarmaceutyków, czyli preparatów medycznych zawierających izotopy promieniotwórcze.
Obecnie _Synektik_ jest krajowym liderem w produkcji i dystrybucji tego typu leków [@synektik].

Jak można zauważyć w powyższym zestawieniu, spółki objęte badaniem istotnie obejmują różne sektory rynku, co ma dodatkowo porównać wpływ zaburzeń na świecie na każdą z dziedzin.
Dodatkowo, jako krajowych liderów w swoich branżach, wyniki finansowe objętych badaniem firm są dobrym wyznacznikiem kondycji całego polskiego rynku.

Celem zróżnicowania badania, procedura badawcza zostanie zastosowana również dla **indeksu WIG20**.
Indeks ten jest indeksem cenowym 20 największych spółek na _Giełdzie Papierów Wartościowych_, obliczanym na podstawie ich obrotów i cen akcji [@wig20stat].

## Analiza danych

```{r load_data}
source("includes/data.r")
```

Do przeprowadzania badania wykorzystane zostaną dane o cenach akcji zebrane z serwisu [Stooq.pl](https://stooq.pl/).
Jest to jeden z najbardziej znanych serwisów internetowych zbierających dane finansowe spółek notowanych nie tylko na _Giełdzie Papierów Wartościowych_, ale również na innych giełdach światowych, m.in. amerykański indeks _S&P US_.
Można tam znaleźć również informacje o kursach walut, cenach towarów i inne dane finansowe.

### Wstępna inspekcja danych

Punktem wyjścia będzie cena zamknięcia akcji.
Aby zapewnić stacjonarność tego szeregu czasowego, obliczane są **logarytmiczne stopy zwrotu** według wzoru:
$$
r_{i} = \ln\left(\frac{x_{i}}{x_{i-1}}\right) \cdot 100
$$
gdzie $x_i$ oznacza cenę zamknięcia w momencie $i$.
Tak utworzony szereg czasowy spełnia założenia stacjonarności i można go wykorzystać do dalszych badań.
W Tabeli \@ref(tab:showcase) zamieszczone zostały przykładowe dane dotyczące indeksu WIG20 wraz z obliczonymi stopami zwrotu.

```{r showcase}
data %>%
  head(5) %>%
  select(-index) %>%
  kable(booktabs = TRUE, format = "latex",
    caption = "Przykład danych giełdowych indeksu WIG20",
    col.names = c(
      "Data",
      "Otwarcie",
      "Najwyższy",
      "Najniższy",
      "Zamknięcie",
      "Wolumen",
      "Stopa zwrotu"
    )
  ) %>%
  kable_styling(latex_options = "HOLD_position")
```

```{r returns-plot, fig.cap="Logarytmiczne stopy zwrotu dla badanych spółek", fig.height=8, fig.width=6.5}
data %>%
  ggplot(aes(Data, return)) +
    geom_line() +
    ylab("Wartość stopy zwrotu") +
    scale_x_date(
      date_breaks = "6 months",
      date_labels = "%m.%y"
    ) +
    facet_wrap(index ~ ., ncol = 1)
```

Na Rysunku \@ref(fig:returns-plot) opracowane są wykresy logarytmicznych stóp zwrotu dla spółek wykorzystanych w badaniu.
Można zauważyć wyraźnie podwyższoną zmienność w marcu 2020 r., a także na końcu lutego 2022 r.
Pierwszy z tych wstrząsów bez wątpienia można przypisać początkowi pandemii koronawirusa w Polsce i pierwszym wprowadzonym restrykcjom sanitarnym [@covidBeginnings].
Drugi z kolei jest związany z początkiem agresji rosyjskiej na Ukrainie, kiedy to 24.02 agresor rozpoczął swoją ofensywę [@ukrWiki].

```{r desc-stats-table}
data %>%
  group_by(index) %>%
  na.omit() %>%
  summarise(
    "Średnia" = mean(return),
    "Mediana" = median(return),
    "Odch. kw." = sd(return),
    "Skośność" = skewness(return),
    "Kurtoza" = kurtosis(return),
  ) %>%
  kable(booktabs = TRUE, format = "latex",
    caption = "Statystyki opisowe stóp zwrotu badanych spółek"
  ) %>%
  kable_styling(latex_options = "HOLD_position")
```

```{r histograms-plot, fig.cap = "Wyniki testu Shapiro-Wilka na normalność rozkładu stóp zwrotu. W tytułach podano wartości statystyki testowej oraz p-wartość."}
plots <- lapply(
  seq_along(tables),
  function(i, data) {
    returns <- data[[i]]
    label <- tables_full_names[names(data)[i]]

    shapiro <- shapiro.test(returns)
    test_results <-
      paste("W:", round(shapiro$statistic, 2), "p:", round(shapiro$p.value, 4))
    returns %>%
      as_tibble() %>%
      na.omit() %>%
      ggplot(aes(value)) +
        geom_histogram(aes(y = ..density..), fill = primary_color, alpha = 0.5) +
        stat_function(
          fun = dnorm,
          args = list(
            mean = mean(returns, na.rm = TRUE),
            sd = sd(returns, na.rm = TRUE)
          ),
          col = primary_color,
        ) +
        labs(
          title = label,
          subtitle = test_results,
          x = NULL,
          y = NULL,
        )
  },
  data = returns_split
)

grid.arrange(
  grobs = plots,
  ncol = 3
)
```

W dalszej kolejności dane giełdowe zostały zbadane pod kątem ich podobieństwa do rozkładu normalnego.
W tym celu obliczone zostały statystyki opisowe szeregów czasowych, a także przeprowadzono test Shapiro-Wilka na normalność rozkładu zmiennej losowej.
Wyniki tych działań przedstawiono w Tabeli \@ref(tab:desc-stats-table) oraz na Rysunku \@ref(fig:histograms-plot).

Na podstawie tak przeprowadzonych testów można stwierdzić, że wzięte w badaniu dane **nie cechują się normalnością**, o czym świadczy niska (bliska wartości 0) p-wartość testów.

## Wybór odpowiedniego modelu

W myśl badania, do omawianych danych finansowych zostały dopasowane różne modele wariancji warunkowej.
Zanim to jednak nastąpi, do danych szeregów czasowych dopasowany zostanie właściwy model średniej warunkowej ARMA oraz przetestowana zostanie obecność efektu ARCH.
W kolejnych podrozdziałach znajduje się opis przeprowadzonych badań oraz zestawienie ich wyników.

### Dopasowanie modelu średniej warunkowej ARMA

Celem dopasowania modelu średniej warunkowej do badanych danych przygotowano korelogramy ilustrujące funkcje ACF i PACF, przedstawione na Rysunku \@ref(fig:arma-acfs).

```{r arma-acfs, fig.height=8, fig.cap="Korelogramy przedstawiające funkcje ACF i PACF dla badanych szeregów czasowych oraz ich kwadratów"}
custom_acf_plot(returns_split)
```

Większość z badanych szeregów czasowych przejawia brak autokorelacji w pierwszych opóźnieniach szeregu.
Wyjątek stanowią stopy zwrotu indeksów:

- 11 bit studios -- wyraźna autokorelacja w opóźnieniach 1., 2. i 5.,
- PKN Orlen -- istotna autokorelacja w opóźnieniu 4.,
- PKO BP -- autokorelacja w opóźnieniach 1. i 5..

Na tej podstawie do wyżej wymienionych danych zbadano dopasowanie modeli ARMA wyższych rzędów.

W przypadku kwadratów szeregów stóp zwrotu wszystkie przejawiają autokorelację co najmniej w pierwszym opóźnieniu.
Występowanie tego typu zależnosci kwadratowych sugeruje zasadność wykorzystania modeli klasy GARCH,
co zostanie dokładniej przetestowane w dalszej części pracy.

Modele ARMA zostały wybrane na podstawie kryteriów informacyjnych Akaike i Schwarza.
Do tego celu wykorzystano pakiet `forecast` [@packageForecast1;@packageForecast2].

```{r arma-fitting}
table <- tibble(series = tables_full_names)

arma <- custom_read_rds("arma_aic")
aic <- map(arma, ~ .x %>% AIC %>% round)
bic <- map(arma, ~ .x %>% BIC %>% round)

table <- table %>% mutate(
  aic_model = c(
    "biały szum",
    "ARMA(2, 2)",
    "ARMA(2, 1)",
    "MA(2)",
    "biały szum",
    "biały szum"
  ),
  aic_aic = aic,
  aic_bic = bic
)

arma <- custom_read_rds("arma_bic")
aic <- map(arma, ~ .x %>% AIC %>% round)
bic <- map(arma, ~ .x %>% BIC %>% round)

table <- table %>% mutate(
  bic_model = rep("biały szum", 6),
  aic = aic,
  bic = bic
)

table %>%
  custom_kable(
    "Zestawienie dopasowanych modeli średniej warunkowej
    do badanych szeregów czasowych,
    wraz z ich kryteriami informacyjnymi",
    col.names = c(
      "Szereg czasowy",
      rep(c(
      "Wybrany model",
      "AIC",
      "BIC"
      ), 2)
    )
  ) %>%
  add_header_above(c(
    " " = 1,
    "Na podst. AIC" = 3,
    "Na podst. BIC" = 3
  ))
```

Tabela \@ref(tab:arma-fitting) pokazuje wyniki przeprowadzonego dopasowania modeli średniej warunkowej do badanych szeregów czasowych na podstawie kryterium informacyjnego Akaike.
Próby dopasowania różnych modeli ARMA w dużej mierze pokrywają się z obserwacjami poczynionymi na podstawie korelogramów na Rysunku \@ref(fig:arma-acfs):
dla danych spółek _PKN Orlen_, _PKO BP_ oraz _11 bit studios_ autokorelacja została uwzględniona w modelach ARMA oraz MA,
z opóźnieniami w dużej mierze zgodnymi z oczekiwaniami.
Dla pozostałych szeregów czasowych model białego szumu jest również adekwatny do braku zaobserwowanych autokorelacji.

Interesującym wynikiem badań jest fakt, że pomimo istotnej autokorelacji występującej w niektórych z badanych szeregów,
jeśli by brać pod uwagę kryterium informacyjne Schwarza do wyłonienia najlepiej dopasowanego modelu,
każdemu z szeregów przypisany zostaje model białego szumu, tj. czysto losowych innowacji.
Najprawdopodobniej powodem takiego wyniku jest bardziej konserwatywne zachowanie się tego kryterium w stosunku do AIC,
które to przypisuje modelowi mniejszą karę za jego skomplikowanie, będące konsekwencją liczby parametrów.
Może to sugerować, że modele wyłonione pod kątem kryterium Akaike są bliskie swojej skuteczności modelom wyłonionym przez kryterium Schwarza.

Mimo to, celem zapewnienia sobie odpowiednich (niezaburzonych) wyników testów na występowanie efektu ARCH,
do dalszych rozważań wybrane zostały modele na podstawie kryterium informacyjnego Akaike,
których specyfikację przedstawia Tabela \@ref(tab:arma-fitting-choice).

```{r arma-fitting-choice}
arma <- custom_read_rds("arma_aic")
coefs <- map(arma, coef)

tibble(
  series = tables_full_names,
  aic_model = c(
    "biały szum",
    "ARMA(2, 2)",
    "ARMA(2, 1)",
    "MA(2)",
    "biały szum",
    "biały szum"
  ),
  ar1 = c(NA, coefs$pkn["ar1"], coefs$pko["ar1"], NA, NA, NA),
  ar2 = c(NA, coefs$pkn["ar2"], coefs$pko["ar2"], NA, NA, NA),
  ma1 = c(NA, coefs$pkn["ma1"], coefs$pko["ma1"], coefs$ebs["ma1"], NA, NA),
  ma2 = c(NA, coefs$pkn["ma2"], NA, coefs$ebs["ma2"], NA, NA)
) %>%
  custom_kable(
    "Zestawienie modeli średniej warunkowej,
    wybranych na podstawie kryterium informacyjnego Akaike,
    wraz z wartościami poszczególnych parametrów",
    col.names = c(
      "Szereg czasowy",
      "Model",
      "$a_1$",
      "$a_2$",
      "$b_1$",
      "$b_2$"
    )
  ) %>%
  add_header_above(c(
    " " = 2,
    "Parametry modelu" = 4
  ))
```

Aby potwierdzić zasadność wykorzystania modeli GARCH przeprowadzone zostały testy na występowanie efektu ARCH.
Należą do nich testy:

- Ljunga-Boxa dla reszt z modelu,
- Engle'a,
- KPSS.

Do obliczenia dwóch ostatnich testów wykorzystane zostaną pakiedy `rugarch` [@packageRugarch] oraz `tseries` [@packageTseries].
Wyniki tych testów ilustruje Rysunek \@ref(fig:arma-arch-acf) oraz Tabela \@ref(tab:arma-arch-tests).

```{r arma-arch-acf, fig.height=8, fig.cap="Korelogramy ilustrujące testy Ljunga-Boxa dla reszt z modelu średniej warunkowej"}
map(arma, residuals) %>%
  custom_acf_plot()
```

Test Ljunga-Boxa, który został przedstawiony na korelogramach, wskazuje na brak autokorelacji w szeregach reszt z modelu.
Dla szeregów kwadratów reszt nadal wyraźnie widoczne są zależności kwadratowe, co pozwala na dalsze rozważanie modelów objaśniających te zależności.

```{r arma-arch-tests}
group_names <- rep(4, length(tables))
names(group_names) <- tables_full_names

map(
  names(arma),
  function(series){
    model <- arma[[series]]

    arch_test <-
      map(
        c(2, 5, 10),
        ~ model %>%
          residuals() %>%
          rugarch:::.archlmtest(.x)
      ) %>%
      sapply(c) %>%
      t() %>%
      as_tibble() %>%
      mutate(
        across(everything(), unlist),
        test = "Engle'a"
      ) %>%
      relocate(test, parameter, .before = statistic) %>%
      select(-method)

    kpss_test <-
      model$x %>%
      kpss.test() %>%
      c() %>%
      as_tibble() %>%
      select(statistic, p.value) %>%
      mutate(
        test = "KPSS",
        parameter = NA
      ) %>%
      relocate(test, parameter, .before = statistic)

    arch_test %>%
      bind_rows(kpss_test)
  }
) %>%
  bind_rows() %>%
  mutate(
    statistic = round(statistic, 2)
  ) %>%
  custom_kable(
    "Wyniki testu Engle'a i KPSS dla badanych szeregów czasowych.
    Dla testów KPSS wartość 0.1 w kolumnie p-wartość oznacza,
    że rzeczywista p-wartość jest wyższa od wskazanej.",
    col.names = c(
      "Test",
      "Opóźnienie",
      "Wart. stat. test.",
      "p-wartość"
    )
  ) %>%
  pack_rows(index = group_names)
```

Wyniki testów Engle'a oraz KPSS również sprzyjają dalszym rozważaniom.
Niskie p-wartości dla pierwszego z nich nakazują odrzucenie hipotezy o braku autokorelacji w kwadratach reszt,
natomiast wysokie (>0.1) p-wartości testu KPSS pozwalają przyjąć hipotezę o heteroskedastyczności zachodzącej w szeregach czasowych.

Wobec tak przeprowadzonego badania dotyczącego modelu średniej warunkowej
możliwym jest przejście do dalszej analizy uwzględniającej dopasowanie modelu wariancji warunkowej.

### Model przełącznikowy Markowa

W testach wzięto pod uwagę modele wariancji warunkowej GARCH, EGARCH oraz GJR, a&nbsp;także różne rozkłady innowacji: normalny, GED, Studenta oraz skośny Studenta.
Daje to 12 kombinacji dla każdego z reżimów, wobec czego w toku badań przetestowano dopasowanie 144 kombinacji modeli.
Zostały one przeanalizowane pod kątem kryteriów informacyjnych Akaike oraz Schwarza, a także pod względem istotności parametrów.
Dla najlepiej dopasowanych modeli przygotowano wizualizacje prawdopodobieństwa przebywania szeregu czasowego w danym reżimie.

W celu dopasowania modelu Markowa do danych zastosowano pakiet `MSGARCH` [@packageMSGARCH].
Oferuje on szeroki wachlarz modeli wariancji warunkowej, w tym dopasowanie odrębnych modeli do poszczególnych reżimów.

Istotnym problemem, jaki należy rozwiązać w przypadku bieżącego badania i wykorzystania wyżej wymienionego pakietu, jest fakt,
że nie obsługuje on specyfikacji modelu średniej warunkowej.
Aby zastosować model przełącznikowy do tego typu danych, konieczne jest dodatkowe przetworzenie danych [@packageMSGARCHDocs, p. 2].
Wobec tego szeregami czasowymi wykorzystywanymi do modelowania wariancji warunkowej są **szeregi czasowe reszt z modelu** średniej warunkowej. \AAQ{czy to dobra droga?}

```{r markov-models}
models_in_list <- 1

map(
  tables,
  function(table_name) {
    models <- custom_read_rds(
      custom_paste_tight(
        "markov_", table_name
      )
    )

    markov_models_for_testing_indices %>%
      mutate(
        series = tables_full_names[table_name],
        model1 = markov_models_for_testing_names[Var1],
        model2 = markov_models_for_testing_names[Var2],
        aic = map(
          models,
          ~ if(class(.x) == "NULL")
            NA
          else
            AIC(.x)
        ) %>% unlist(),
        bic = map(
          models,
          ~ if(class(.x) == "NULL")
            NA
          else BIC(.x)
        ) %>% unlist()
      ) %>%
      arrange(aic) %>%
      select(-Var1, -Var2) %>%
      head(models_in_list)
  }
) %>%
  bind_rows() %>%
  custom_kable(
    "Najlepsze wartości kryteriów informacyjnych dla modeli Markowa
    dopasowanych do stóp zwrotu badanych spółek",
    col.names = c(
      "Szereg czasowy",
      "Model reżimu 1",
      "Model reżimu 2",
      "AIC",
      "BIC"
    )
  ) %>%
  add_header_above(
    c(" " = 3, "Kryterium informacyjne" = 2)
  )
```

\AAA{ten wykres się jeszcze przyda}

```{r markov, fig.cap="Dopasowanie modelu Markowa do danych WIG 20", fig.height=4}
fit <- FitML(
  CreateSpec(
    variance.spec = list(model = c("sGARCH", "sGARCH")),
    distribution.spec = list(distribution = c("norm", "norm")),
    switch.spec = list(do.mix = FALSE, K = NULL)
  ),
  wig20 %>% pull(return) %>% na.omit()
)

custom_markov_plot(
  wig20 %>% select(Data, return),
  State(fit)$SmoothProb[, 1, 2]
)
```

### GARCHX

W celu dopasowania modelu GARCHX koniecznym jest przygotowanie dodatkowych zmiennych, mających na celu objaśnienie wpływu czynników zewnętrznych na zmiennośc wariancji warunkowej.

> - na podstawie czego mógłbym to zbudować?
> - może dla każdej branży coś innego dodatkowego? jakieś specyficzne wydarzenia?

Podobnie jak w poprzednim podrozdziale, dla każdego z badanych szeregów czasowych dopasowano model GARCHX z uwzględnieniem dodatkowych zmiennych.
Następnie, na podstawie kryteriów informacyjnych Akaike i Schwarza, a także instotności parametrów modelu, wyłonione zostały te najlepiej dopasowane.
Dla nich sporządzone zostały odpowiednie wizualizacje.

Pakietem wykorzystanym do tej części badań jest pakiet

> `garchx`? chyba ten jest najpopularniejszy

## Predykcje

Posiadając odpowiednie modele dopasowane do danych, możliwym jest wykonanie prognoz ex-post i porównanie ich z rzeczywistym kształtowaniem się wartości odpowiadających indeksów giełdowych.
\AAA{jak daleka ta prognoza?}

> - prognozy stóp zwrotu
> - reverse-engineering tych stóp na wartości akcji
> - wykresy dla każdego indeksu
 