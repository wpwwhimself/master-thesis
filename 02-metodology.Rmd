# Metodologia badawcza

```{=latex}
\fancyhead[LO]{\textbf{\small{Metodologia badawcza}}}
\renewcommand{\headrulewidth}{0.5pt}
```

## Wybór danych

Za przedmiot badań obrane zostały dane pięciu polskich spółek notowanych na&nbsp;Giełdzie Papierów Wartościowych.
Spółki te zostały wybrane tak, aby reprezentowały one różne sektory rynku.
Poddano badaniu następujące firmy:

1. **PKN Orlen** -- Polski Koncern Naftowy Orlen Spółka Akcyjna -- polska spółka z ponad sześćdziesięcioletnią historią w **branży paliwowej**.
Opisywana jako koncern wielobranżowy, opiera swoją działalność na&nbsp;poszukiwaniu, wydobyciu, logistyce oraz dystrybucji i sprzedaży różnego rodzaju paliw, m.in. ropy naftowej, gazu ziemnego oraz biopaliw.
Ponadto spółka udziela się na&nbsp;rynkach energii elektrycznej i cieplnej, produkcji nawozów sztucznych, petrochemii, a&nbsp;także na&nbsp;rynku prasowym.
Pod szyldem _Orlen_ działa 7 rafinerii w Polsce, Czechach oraz&nbsp;na&nbsp;Litwie, które przerabiają rocznie ponad 30 mln ton ropy naftowej.
Spółka posiada ponad 3000 stacji benzynowych w Europie i sprzedaje 40 produktów petrochemicznych [@orlenWiki;@orlenSelf].

2. **PKO BP** -- Powszechna Kasa Oszczędności Bank Polski Spółka Akcyjna -- największy bank uniwersalny w Polsce, reprezentujący **branżę finansową**.
Utworzona 7.02.2019 r. jako _Pocztowa Kasa Oszczędności_, która przyczyniła się m.in. do wprowadzenia złotego polskiego do obiegu w Polsce.
Po 1948 r. zlikwidowana i powołana ponownie, tym razem jako bank państwowy (już pod obecną nazwą), a z dniem 1.07.1975 r. włączona w struktury _Narodowego Banku Polskiego_.
Jako bank uniwersalny obsługuje różne podmioty, od osób fizycznych, poprzez małe i średnie przedsiębiorstwa, skończywszy na&nbsp;wielkich korporacjach.
Misją banku jest pozytywny wpływ na&nbsp;Polskę, tj. ludzi, firmy, kulturę i środowisko. Określany jako jeden z największych banków w Europie Środkowej i Wschodniej.
W posiadaniu spółki jest również kolekcja różnego rodzaju dzieł sztuki, a pod jej szyldem funkcjonuje również _Fundacja_, realizująca m.in. projekty lokalne i indywidualne [@pkoWiki;@pkoSelf].

3. **11 bit studios** -- polskie przedsiębiorstwo założone w 2009 roku z siedzibą w Warszawie, zajmującą się produkcją i wydawaniem **gier komputerowych**.
Założone w większości przez byłych pracowników _Metropolis Software_, innego polskiego producenta gier, który w wyniku braku sukcesów na&nbsp;rynku zmuszony był zakończyć swoją działalność.
Nowo utworzone studio wydało kilka produkcji, z czego szczególnym sukcesem cieszy się gra strategiczna z&nbsp;gatunku _city builder_: _Frostpunk_.
Będąc laureatem wielu wyróżnień, m.in. na&nbsp;najlepszą grę strategiczną podczas _Video Game Awards_ oraz najlepszą grę polską od _Digital Dragons_, studio stale rozszerza swój dorobek, pracując obecnie nad kontynuacją swojej hitowej produkcji.
Produkcję gier traktują na&nbsp;równi z twórczością artystyczną, chcąc konstruować ,,uniwersa pełne prowokujących do&nbsp;myślenia powieści" [@11bitWiki;@11bitSelf].

4. **Dom Development** -- spółka akcyjna działająca od 1996 roku, zajmująca się nabywaniem gruntów i budowaniem, sprzedażą oraz najmem **nieruchomości** o przeznaczeniu mieszkaniowym.
Spółka jest notowana od 2006 r. na&nbsp;warszawskiej Giełdzie Papierów Wartościowych.
Jest laureatem wielu nagród i wyróżnień, m.in. _Perła Polskiej Gospodarki 2008_, _Diament Forbesa 2007_ oraz _Top 100 Najefektywniejszych Firm 2006 r_.
W 2018 r. powołała do życia spółkę _Dom Construction Sp. z o.o._ o charakterze generalnego wykonawcy swoich inwestycji.
Wykonuje projekty budowlane w największych miastach Polski: Warszawie, Wrocławiu, Gdańsku i Krakowie.
W ciągu ponad 25 lat działalności firma oddała do użytku ponad 46 tys. lokali [@domSelf].

1. **Synthaverse** -- polska spółka akcyjna dostarczająca wiodące na&nbsp;rynku rozwiązania **farmaceutyczne** z niemal stuletnią tradycją.
Rozpoczęła działalność jako _Laboratorium Produkcji Szczepionki przeciwko Durowi Plamistemu_ oraz _Zakład Produkcji Surowic i Szczepionek przy&nbsp;Filii PZH w Lublinie_,
a na&nbsp;giełdzie przez długi czas notowana była jako _Biomed Lublin S.A._.
Dostarcza swoje produkty do 50 krajów świata, a swoją działalność koncentruje na&nbsp;rozwijaniu immunoterapii.
Do jej szczególnych osiągnięć należy status producenta jednego z nielicznych leków stosowanych w terapii raka pęcherza moczowego, a także powszechnie stosowana szczepionka przeciwgruźlicza.
Opracowała również _Lakcid_ -- probiotyk, będący jednym z niewielu tego rodzaju leków dostępnych na&nbsp;świecie.
Obecnie _Synthaverse_ jest krajowym liderem w produkcji i dystrybucji tego typu leków [@synthaverseSelf].

### Zasadność wyboru

Jak można zauważyć w powyższym zestawieniu, spółki objęte badaniem istotnie obejmują różne sektory rynku, co ma pomóc w porównaniu wpływu wydarzeń na&nbsp;świecie na&nbsp;każdą z dziedzin gospodarki.
Dodatkowo, jako że mowa o krajowych liderach w swoich branżach, wyniki finansowe objętych badaniem firm są prawdopodobnie dobrym wyznacznikiem kondycji całego polskiego rynku.
Skala prowadzonego badania niestety nie pozwoliła na&nbsp;wybranie większej liczby sektorów rynku, ograniczając się jedynie do pięciu.
Możliwym ryzykiem takiego podejścia jest błąd we wnioskowaniu o stanie całej gospodarki Polski na&nbsp;podstawie tak ograniczonej puli firm, których dane są brane pod&nbsp;uwagę.

W tym celu, aby dodatkowo zobrazować reakcję rynku na&nbsp;zmiany globalne, procedurę badawczą zastosowano również dla **indeksu giełdowego WIG20**.
Indeks ten jest indeksem cenowym 20&nbsp;największych spółek na&nbsp;_Giełdzie Papierów Wartościowych_, obliczanym na&nbsp;podstawie ich obrotów i&nbsp;cen akcji [@wig20stat].
Do tego indeksu należą dwie z badanych spółek: _PKN&nbsp;Orlen_ oraz _PKO BP_.
Pozostałe trzy natomiast konstytuują skład dwóch innych indeksów giełdowych:
kolejno _mWIG40_ dla _11 bit studios_ i _Dom Development_ oraz _sWIG80_ dla _Synthaverse_.
Na&nbsp;jednym z&nbsp;etapów badania, logarytmiczne stopy zwrotu indeksu *WIG20* zostały wykorzystane jako dodatkowa zmienna objaśniająca na&nbsp;potrzeby modeli GARCH-X,
celem próby objaśnienia zmienności stóp zwrotu zmianami na&nbsp;całym rynku.

Zgodnie z powyższymi dywagacjami, to podejście może przynieść nieoczekiwane rezultaty, z&nbsp;uwagi na&nbsp;brak udziału trzech z pięciu spółek w składzie tego indeksu.
Niemniej prawdopodobnym jest, że skala zmian ogólnoświatowych jest wystarczająco duża, że swoim wpływem pokrywa również mniejsze przedsiębiorstwa.
Niewątpliwie tak było w przypadku pandemii koronawirusa, jednak dalsze badania są konieczne w celu potwierdzenia tej tezy.

## Analiza danych

```{r load_data}
source("includes/data.r")
```

Do przeprowadzania badania wykorzystane zostaną dane o cenach akcji zebrane z serwisu _[Stooq.pl](https://stooq.pl/)_.
Jest to jeden z najbardziej znanych serwisów internetowych zbierających dane finansowe spółek notowanych nie tylko na&nbsp;_Giełdzie Papierów Wartościowych_, ale również na&nbsp;innych giełdach światowych, m.in. amerykański indeks _S&P US_.
Można tam znaleźć również informacje o kursach walut, cenach towarów i inne dane finansowe.
Usługi serwisu są skierowane głównie do osób biegłych w rynkach finansowych, poszukujących atrakcyjnych form lokowania kapitału, ale także menadżerów, biznesmenów oraz właścicieli firm [@stooq].
Na&nbsp;potrzeby badania pobrano dane giełdowe z okresu `r data_range[1]` - `r data_range[2]`,
co daje `r length(tables)` `r length(na.omit(returns_split[[1]]))`-elementowych szeregów czasowych.

Punktem wyjścia do prowadzenia badań będzie wybranie szeregu czasowego cen zamknięcia akcji.
Aby zapewnić stacjonarność tego szeregu czasowego, obliczane są logarytmiczne stopy zwrotu według Wzoru \@ref(eq:return).
Tak utworzony szereg czasowy spełnia założenia stacjonarności i można go wykorzystać do dalszych badań.
W celach poglądowych w Tabeli \@ref(tab:showcase) zamieszczone zostały przykładowe dane dotyczące indeksu WIG20 wraz z obliczonymi stopami zwrotu.

```{r showcase}
data_split$wig %>%
  head(5) %>%
  select(-is_crisis) %>%
  c_kable(
    "Przykład danych giełdowych indeksu WIG20",
    col.names = c(
      "Data",
      "Otwarcie",
      "Najwyższy",
      "Najniższy",
      "Zamknięcie",
      "Wolumen",
      "Stopa zwrotu"
    )
  )
```

```{r stock-prices-plot, fig.cap="Ceny zamknięcia akcji oraz wartości indeksu dla badanych spółek", fig.height=8, fig.width=6.5}
data_split %>%
  bind_rows(.id = "index") %>%
  mutate(index = tables_full_names[index]) %>%
  select(dates = Data, values = Zamkniecie, index) %>%
  c_line_plot_multi("Wartość instrumentu")
```

```{r returns-plot, fig.cap="Logarytmiczne stopy zwrotu dla badanych spółek", fig.height=8, fig.width=6.5}
data_split %>%
  bind_rows(.id = "index") %>%
  mutate(index = tables_full_names[index]) %>%
  select(dates = Data, values = return, index) %>%
  c_line_plot_multi("Wartość stopy zwrotu")
```

Na&nbsp;Rysunku \@ref(fig:returns-plot) opracowane są wykresy logarytmicznych stóp zwrotu dla spółek wykorzystanych w badaniu.
Można zauważyć wyraźnie podwyższoną zmienność w marcu 2020 r., a także na&nbsp;końcu lutego 2022 r.
Pierwszy z tych wstrząsów bez wątpienia można przypisać początkowi pandemii koronawirusa w Polsce i pierwszym wprowadzonym restrykcjom sanitarnym.
Drugi z kolei jest związany z początkiem agresji rosyjskiej na&nbsp;Ukrainie, kiedy to 24.02 agresor rozpoczął swoją ofensywę [@covidBeginnings;@ukraineWiki].

```{r desc-stats-table}
data_split %>%
  bind_rows(.id = "index") %>%
  mutate(index = tables_full_names[index]) %>%
  group_by(index) %>%
  na.omit() %>%
  summarise(
    mean(return),
    median(return),
    sd(return),
    skewness(return),
    kurtosis(return),
  ) %>%
  c_kable(
    "Statystyki opisowe stóp zwrotu badanych spółek",
    col.names = c(
      "Szereg czasowy",
      "Średnia",
      "Mediana",
      "Odch. kw.",
      "Skośność",
      "Kurtoza"
    ),
    linesep = ""
  )
```

```{r histograms-plot, fig.cap = "Wyniki testu Shapiro-Wilka na normalność rozkładu stóp zwrotu"}
plots <- lapply(
  tables,
  function(table_name, i) {
    returns <- returns_split[[table_name]]
    label <- tables_full_names[table_name]

    shapiro <- shapiro.test(returns)
    test_results <- paste(
      "W:", round(shapiro$statistic, 2),
      "p:", c_format_pval(shapiro$p.value)
    )
    returns %>%
      as_tibble() %>%
      na.omit() %>%
      ggplot(aes(value)) +
        geom_histogram(aes(y = ..density..), fill = primary_color, alpha = 0.5) +
        stat_function(
          fun = dnorm,
          args = list(
            mean = mean(returns, na.rm = TRUE),
            sd = sd(returns, na.rm = TRUE)
          ),
          col = primary_color,
        ) +
        labs(
          title = label,
          subtitle = test_results,
          x = NULL,
          y = NULL,
        )
  }
)

grid.arrange(
  grobs = plots,
  ncol = 3
)
```

W dalszej kolejności dane giełdowe zostały zbadane pod kątem ich podobieństwa do rozkładu normalnego.
W tym celu obliczone zostały statystyki opisowe szeregów czasowych, a także przeprowadzono test Shapiro-Wilka na&nbsp;normalność rozkładu zmiennej losowej.
Wyniki tych działań przedstawiono w Tabeli \@ref(tab:desc-stats-table) oraz na&nbsp;Rysunku \@ref(fig:histograms-plot).
Na&nbsp;podstawie tak przeprowadzonych testów można stwierdzić, że wzięte w badaniu dane **nie wykazują cech rozkładu normalnego**, o czym świadczy niska (bliska wartości 0) p-wartość testów.
Uzasadnia to zastosowanie modeli rozważanych w badaniach.

## Wybór odpowiedniego modelu

W myśl badania, do omawianych danych finansowych zostały dopasowane różne modele wariancji warunkowej.
Zanim jednak wykonano ten proces, aby zapewnić sobie modele w pełni objaśniające dane rzeczywiste,
do danych szeregów czasowych dopasowano właściwy model średniej warunkowej ARMA oraz przetestowano je na&nbsp;obecność efektu ARCH.
W kolejnych podrozdziałach znajduje się opis przeprowadzonych badań oraz zestawienie ich wyników.

### Dopasowanie modelu średniej warunkowej ARMA

```{r arma-acfs, fig.height=8, fig.cap="Korelogramy przedstawiające funkcje ACF i PACF dla badanych szeregów czasowych"}
c_acf_plot(returns_split)
```

W celu dopasowania modelu średniej warunkowej do badanych danych,
przygotowano korelogramy ilustrujące funkcje ACF i PACF, przedstawione na&nbsp;Rysunku \@ref(fig:arma-acfs).
Posłużyły one do wskazania istotnych opóźnień autokorelacji w badanych szeregach czasowych i pozwoliły na&nbsp;określenie zakresu testowanych parametrów.
Połowa badanych szeregów czasowych przejawia brak autokorelacji w&nbsp;pierwszych opóźnieniach szeregu,
natomiast dla trzech z nich testy przyniosły wynik pozytywny.
Na&nbsp;tej podstawie podczas testów dopasowania zbadano dopasowanie modeli ARMA do maksymalnego opóźnienia rzędu `r arma_max_lag`.
Modele zostały wybrane na&nbsp;podstawie kryteriów informacyjnych Akaike i&nbsp;Schwarza.
Do tego celu wykorzystano pakiet `forecast` [@packageForecast1;@packageForecast2].

```{r arma-fitting}
table <- tibble(series = tables_full_names)


arma <- c_read_rds("arma_aic")
table <- table %>% mutate(
  aic_model = map(arma, c_get_arma_name),
  aic_aic = map(arma, ~ .x %>% AIC %>% round),
  aic_bic = map(arma, ~ .x %>% BIC %>% round)
)

arma <- c_read_rds("arma_bic")
table <- table %>% mutate(
  bic_model = map(arma, c_get_arma_name),
  bic_aic = map(arma, ~ .x %>% AIC %>% round),
  bic_bic = map(arma, ~ .x %>% BIC %>% round)
)

table %>%
  c_kable(
    "Zestawienie dopasowanych modeli średniej warunkowej
    do badanych szeregów czasowych,
    wraz z ich kryteriami informacyjnymi",
    col.names = c(
      "Szereg czasowy",
      rep(c(
      "Wybrany model",
      "AIC",
      "BIC"
      ), 2)
    ),
    linesep = ""
  ) %>%
  add_header_above(c(
    " " = 1,
    "Na podst. AIC" = 3,
    "Na podst. BIC" = 3
  ))
```

Tabela \@ref(tab:arma-fitting) pokazuje wyniki przeprowadzonego dopasowania modeli średniej warunkowej do badanych szeregów czasowych na&nbsp;podstawie kryteriów informacyjnego Akaike oraz Schwarza.
Próby dopasowania różnych modeli ARMA w dużej mierze pokrywają się z obserwacjami poczynionymi na&nbsp;podstawie korelogramów na&nbsp;Rysunku \@ref(fig:arma-acfs):
dla danych spółek _11 bit studios_ oraz _Synthaverse_ autokorelacja została uwzględniona w modelach ARMA,
z opóźnieniami w dużej mierze zgodnymi z&nbsp;oczekiwaniami.
Dla pozostałych szeregów czasowych model białego szumu jest również adekwatny do braku zaobserwowanej autokorelacji.

Interesującym wynikiem badań jest fakt, że pomimo istotnej autokorelacji występującej w niektórych z badanych szeregów,
jeśli by brać pod uwagę kryterium informacyjne Schwarza do wyłonienia najlepiej dopasowanego modelu,
każdemu z szeregów przypisany został model białego szumu, tj.&nbsp;czysto losowych innowacji.
Najprawdopodobniej powodem takiego wyniku jest bardziej konserwatywne zachowanie się tego kryterium w stosunku do AIC,
które to przypisuje modelowi mniejszą karę za jego skomplikowanie, będące konsekwencją liczby jego parametrów.
Może to sugerować, że&nbsp;modele wyłonione pod kątem kryterium Akaike są bliskie swojej skuteczności modelom wyłonionym przez kryterium Schwarza.

Mimo to przeprowadzenie testów Ljung-Boxa na&nbsp;występowanie autokorelacji w szeregach czasowych reszt z modelu dla tych wybranych na&nbsp;podstawie BIC wykazało fakt,
że nie wszystkie zależności liniowe są wyjaśniane przez model białego szumu,
na&nbsp;co wskazuje nadal występująca autokorelacja dla spółek wspomnianych na&nbsp;początku tego rozdziału, związana z tymi samymi opóźnieniami.
Wobec tych obserwacji, stawiając sobie za cel spójność wyników każdego z przeprowadzonych w&nbsp;obrębie tego badania eksperymentów,
do dalszych rozważań wybrane zostały modele na&nbsp;podstawie kryterium informacyjnego Akaike,
których specyfikację przedstawia Tabela \@ref(tab:arma-fitting-choice).

```{r arma-fitting-choice}
arma <- c_read_rds("arma_aic")

table <- tibble(
  series = tables_full_names,
  model = map(arma, c_get_arma_name),
  ar1 = map_dbl(arma, ~ coef(.x)["ar1"]),
  ar2 = map_dbl(arma, ~ coef(.x)["ar2"]),
  ar3 = map_dbl(arma, ~ coef(.x)["ar3"]),
  ma1 = map_dbl(arma, ~ coef(.x)["ma1"]),
  ma2 = map_dbl(arma, ~ coef(.x)["ma2"]),
  ma3 = map_dbl(arma, ~ coef(.x)["ma3"])
)

table %>%
  c_kable(
    "Zestawienie modeli średniej warunkowej,
    wybranych na podstawie kryterium informacyjnego Akaike,
    wraz z wyestymowanymi wartościami poszczególnych parametrów",
    col.names = model_params[colnames(.)],
    linesep = ""
  ) %>%
  add_header_above(c(
    " " = 2,
    "Parametry modelu" = length(colnames(table)) - 2
  ))
```

Aby potwierdzić zasadność wykorzystania modeli GARCH, przeprowadzone zostały testy na&nbsp;występowanie efektu ARCH.
Należą do nich testy:

- McLeoda-Li (test Ljung-Boxa dla kwadratów reszt z modelu) [@mcLeodLiTest],
- Engle'a [@engleARCH],
- KPSS [@KPSSTest].

Obliczenia odpowiednich testów zostały wykonane m.in. za pomocą pakietów `rugarch` [@packageRugarch] oraz `tseries` [@packageTseries].
Wyniki tych testów ilustruje Rysunek \@ref(fig:arma-arch-acf) oraz Tabela \@ref(tab:arma-arch-tests).

```{r arma-arch-acf, fig.height=8, fig.cap="Korelogramy ilustrujące testy McLeoda-Li dla rozważanych modeli średniej warunkowej"}
arma %>%
  map(~ .x %>%
    residuals %>%
    .^2
  ) %>%
  c_acf_plot()
```

```{r arma-arch-tests}
group_names <- rep(4, length(tables))
names(group_names) <- tables_full_names

map(
  names(arma),
  function(series){
    model <- arma[[series]]

    arch_test <-
      map(
        c(2, 5, 10),
        ~ model %>%
          residuals() %>%
          rugarch:::.archlmtest(.x)
      ) %>%
      sapply(c) %>%
      t() %>%
      as_tibble() %>%
      mutate(
        across(everything(), unlist),
        test = "Engle'a"
      ) %>%
      relocate(test, parameter, .before = statistic) %>%
      select(-method)

    kpss_test <-
      model$x %>%
      kpss.test() %>%
      c() %>%
      as_tibble() %>%
      select(statistic, p.value) %>%
      mutate(
        test = "KPSS",
        parameter = NA
      ) %>%
      relocate(test, parameter, .before = statistic)

    arch_test %>%
      bind_rows(kpss_test)
  }
) %>%
  bind_rows() %>%
  mutate(
    p.value = if_else(p.value == 0.1, ">0.1000", c_format_pval(p.value)),
    statistic = round(statistic, 2)
  ) %>%
  c_kable(
    "Wyniki testu Engle'a i KPSS dla badanych szeregów czasowych",
    col.names = c(
      "Test",
      "Opóźnienie",
      "Wart. stat. test.",
      "p-wartość"
    )
  ) %>%
  pack_rows(index = group_names)
```

Test McLeoda-Li, który został przedstawiony na&nbsp;korelogramach na&nbsp;Rysunku \@ref(fig:arma-arch-acf),
wskazuje na&nbsp;istotną autokorelację w szeregach kwadratów reszt z modelu,
wobec czego można wnioskować o zachodzeniu efektu ARCH w rozważanych danych.
Wyniki testów Engle'a oraz KPSS również sprzyjają dalszym rozważaniom -- niskie p-wartości dla pierwszego z nich nakazują odrzucenie hipotezy o braku autokorelacji w kwadratach reszt,
natomiast wysokie (tj. $p>0.1$) p-wartości testu KPSS nie negują hipotezy o heteroskedastyczności zachodzącej w szeregach czasowych.
Wobec tak przeprowadzonego badania dotyczącego modelu średniej warunkowej możliwym jest przejście do dalszej analizy uwzględniającej dopasowanie modelu wariancji warunkowej.

### Model przełącznikowy Markowa

W testach wzięto pod uwagę modele wariancji warunkowej GARCH, EGARCH oraz GJR, a&nbsp;także różne rozkłady innowacji: normalny, GED, Studenta oraz skośny Studenta.
Kombinacje powyższych dają 12 różnych modeli, które zostały przeanalizowane pod kątem kryteriów informacyjnych Akaike oraz Schwarza.
Dla najlepiej dopasowanych modeli przygotowano wizualizacje prawdopodobieństwa przebywania szeregu czasowego w danym reżimie.
W celu dopasowania modelu Markowa do danych zastosowano pakiet `MSGARCH` [@packageMSGARCH].

```{r}
#### see calculations.r // markov ####
```

Istotnym problemem, jaki należy rozwiązać w przypadku bieżącego badania i wykorzystania wyżej wymienionego pakietu, jest fakt,
że nie obsługuje on specyfikacji modelu średniej warunkowej.
Aby zastosować model przełącznikowy do tego typu danych, konieczne jest dodatkowe ich przetworzenie [@packageMSGARCHDocs, s. 2].
Wobec tego szeregami czasowymi wykorzystywanymi do modelowania wariancji warunkowej są **szeregi czasowe reszt z modelu** średniej warunkowej.

```{r markov-models}
stats <-
  map(
    tables,
    function(table_name) {
      models <- c_read_rds(
        c_paste_tight(
          "markov_", table_name
        )
      )

      pb <- c_progress_setup(paste("Checking", table_name), length(models))

      calculated <-
        models %>%
        map(function(model) {
          pb$tick()

          if (class(model) == "NULL") {
            return(list(aic = NA, bic = NA, p1 = NA, p2 = NA))
          }

          list(
            aic = AIC(model),
            bic = BIC(model),
            p1 = c_markov_get_params(model) %>%
              filter(param == "P_1_1") %>%
              pull(Estimate),
            p2 = c_markov_get_params(model) %>%
              filter(param == "P_2_1") %>%
              pull(Estimate)
          )
        }) %>%
        list_transpose()

      tibble(
        series = tables_full_names[table_name],
        model = models_for_testing_names,
        aic = calculated$aic,
        bic = calculated$bic,
        p1 = calculated$p1,
        p2 = calculated$p2
      )
    }
  )

best <-
  stats %>%
  map(~ .x %>%
    mutate(id = seq_len(nrow(.x))) %>%
    arrange(aic) %>%
    head(1) %>%
    pull(id)
  ) %>%
  setNames(tables)

seq_along(stats) %>%
  map(function(stat_id) {
    stats[[stat_id]][
      best[[stat_id]],
    ]
  }) %>%
  bind_rows() %>%
  c_kable(
    "Najlepsze wartości kryteriów informacyjnych dla modeli Markowa
    dopasowanych do stóp zwrotu badanych spółek",
    col.names = c(
      "Szereg czasowy",
      "Model",
      "AIC",
      "BIC",
      "$p_{11}$",
      "$p_{21}$"
    ),
    linesep = ""
  ) %>%
  add_header_above(
    c(" " = 2, "Kryterium informacyjne" = 2, "Prawdop. reżimu" = 2)
  )
```

Tabela \@ref(tab:markov-models) zawiera informacje na&nbsp;temat modeli dopasowanych do badanych szeregów czasowych,
m.in. wartości ich kryteriów informacyjnych, a także prawdopodobieństwa, że szereg czasowy utrzyma się w reżimie nr 1 $p_{11}$ oraz że powróci do niego z reżimu nr 2 $p_{21}$.
Najlepsze z modeli zostały wyłonione na&nbsp;podstawie kryterium informacyjnego Akaike, konsekwentnie z wyborem poczynionym w poprzednim rozdziale.
Można zauważyć, że optymalne wyniki wykazuje model EGARCH, uwzględniający asymetrię wartości szeregu czasowego.
Badanie wykazało też brak konsensusu dotyczącego przyjętego rozkładu innowacji -- każdy z testowanych rozkładów pojawia się na&nbsp;liście co najmniej raz.

```{r markov-re-fit}
# ze względu na błąd przy odczycie .rds, muszę ponownie fitować szeregi
pb <- c_progress_setup(paste("Refitting"), length(tables))
markov <-
  map(
    tables,
    function(table_name) {
      pb$tick()

      series <-
        arma[[table_name]] %>%
        residuals()

      CreateSpec(
        variance.spec = list(model = rep(
          models_for_testing[best[[table_name]],]$volatility,
          2
        )),
        distribution.spec = list(distribution = rep(
          models_for_testing[best[[table_name]],]$distribution,
          2
        )),
        switch.spec = list(do.mix = FALSE, K = NULL)
      ) %>%
        FitML(series)
    }
  ) %>%
    setNames(tables)
```

```{r markov-optimizing, eval = F}
# suspended due to uselessness
markov <-
  map(
    tables,
    function(table_name) {
      message("Checking ", tables_full_names[table_name])
      model <- markov[[table_name]]

      params_to_remove <- c_markov_get_insignif(model)

      if (length(params_to_remove) > 0) {
        message("- Refitting: found ", paste(params_to_remove, sep = ", "))
        model <-
          CreateSpec(
            variance.spec = list(model = rep(
              models_for_testing[best[[table_name]],]$volatility,
              2
            )),
            distribution.spec = list(distribution = rep(
              models_for_testing[best[[table_name]],]$distribution,
              2
            )),
            switch.spec = list(do.mix = FALSE, K = NULL),
            constraint.spec = list(
              fixed = c_list_with_names(
                params_to_remove,
                rep(0, length(params_to_remove))
              )
            )
          ) %>%
            FitML(
              arma[[table_name]] %>%
                residuals()
            )
      }

      return(model)
    }
  ) %>%
    setNames(tables)
```

Ostateczne wartości wyestymowanych parametrów wybranych modeli przedstawia Tabela \@ref(tab:markov-post-optimizing),
a wizualizację prawdopodobieństw zachodzenia poszczególnych reżimów dla każdego z szeregów czasowych prezentuje Rysunek \@ref(fig:markov-plots).

```{r markov-post-optimizing}
group_names <- rep(2, length(tables))
names(group_names) <- tables_full_names

table <- map(
  tables,
  function(table_name) {
    markov[[table_name]] %>%
      c_markov_get_params(with_probs = FALSE) %>%
      select(param, Estimate) %>%
      mutate(
        series = tables_full_names[table_name],
        model = markov[[table_name]]$spec$name[1] %>% str_replace_all("_", " ")
      )
  }
) %>%
  bind_rows() %>%
  separate_wider_delim(param, "_", names = c("param", "regime")) %>%
  pivot_wider(
    names_from = param,
    values_from = Estimate
  ) %>%
  mutate(regime = as.numeric(regime)) %>%
  relocate(regime, .before = alpha0) %>%
  select(-series)

table %>%
  c_kable(
    "Parametry modeli Markowa dopasowanych do stóp zwrotu badanych spółek",
    col.names = model_params[colnames(.)],
    digits = 2
  ) %>%
  pack_rows(index = group_names) %>%
  add_header_above(c(
    " " = 2,
    "Parametry modelu" = length(colnames(table)[colnames(table) %in% c(
      "alpha0", "alpha1", "alpha2", "beta"
    )]),
    "Par. rozkł. inn." = length(colnames(table)[colnames(table) %in% c(
      "nu", "xi"
    )])
  ))
```

```{r markov-plots, fig.cap="Prawdopodobieństwa obecności szeregów czasowych w odpowiednich reżimach dla modelu Markowa na tle modelowanej zmienności warunkowej", fig.height=8, fig.width=6.5}
map(
  tables,
  function(table_name) {
    data_split[[table_name]] %>%
      select(Data, return) %>%
      mutate(
        return = abs(return) / max(abs(return), na.rm = TRUE),
        series = tables_full_names[table_name],
        probs = State(markov[[table_name]])$SmoothProb[, 1, 2]
      )
  }
) %>%
  bind_rows() %>%
  ggplot(aes(Data)) +
    geom_line(aes(y = return), col = "black", alpha = 0.25) +
    geom_area(aes(y = probs), fill = primary_color, alpha = 0.5) +
    labs(
      x = "Data",
      y = "Prawdopodobieństwo reżimu 2"
    ) +
    scale_x_date(
      date_breaks = "4 months",
      date_minor_breaks = "1 month",
      date_labels = "%m.%y"
    ) +
    theme(
      panel.grid.major.x = element_line(color = "gray", size = 0.25)
    ) +
    facet_wrap(vars(series), ncol = 1)
```

### GARCH-X

W celu dopasowania modelu GARCH-X koniecznym jest przygotowanie dodatkowych zmiennych, mających na&nbsp;celu objaśnienie **wpływu czynników zewnętrznych** na&nbsp;zmienność wariancji warunkowej.
Ponieważ badanie rozważa wpływ dwóch konkretnych wydarzeń na&nbsp;arenie międzynarodowej na&nbsp;polską gospodarkę,
za zmienne objaśniające przyjęte zostały 2 szeregi czasowe.
Pierwszym z nich jest zmienna indykatorowa,
wyróżniająca istotne okresy czasu, związane z tymi, wydarzeniami na&nbsp;scenie ekonomicznej Polski.
Wielkość interwałów czasowych została określona w oparciu o Rysunki \@ref(fig:returns-plot) oraz \@ref(fig:markov-plots).
Wyróżnionymi okresami są:

```{r garchx-externals, results='asis'}
cat("\\begin{itemize}\n\\tightlist\n")
walk(garchx_periods, ~ cat(paste(
  "\\item",
  .x[1],
  "-",
  .x[2],
  "r.:",
  .x[3],
  "\n"
)))
cat("\\end{itemize}")
```

Drugą z przyjętych zmiennych jest szereg logarytmicznych stóp zwrotu indeksu WIG20.
Idea tej decyzji mówi, że nawet jeśli dana spółka nie jest składową tego indeksu,
to za pomocą jego wartości możliwym jest wpłynięcie na&nbsp;model danych w taki sam sposób, w jaki na&nbsp;kondycję większych przedsiębiorstw wpłynęły wydarzenia kryzysowe.
Zaobserwowane wahania zmienności WIG20 będą wówczas emulowały czynniki zewnętrzne dla każdej z analizowanych firm.

Podobnie jak w poprzednim podrozdziale, dla każdego z badanych szeregów czasowych dopasowano model GARCH z uwzględnieniem dodatkowych zmiennych objaśniających
-- pod uwagę zostały wzięte te same modele, jak w poprzednim etapie badania, tj. GARCH, EGARCH oraz GJR, z różnymi rozkładami innowacji: normalnym, GED, Studenta i skośnym Studenta.
Następnie, na&nbsp;podstawie kryteriów informacyjnych Akaike i Schwarza wyłonione zostały te najlepiej dopasowane.
Pakietem wykorzystanym do tej części badań jest pakiet `rugarch` [@packageRugarch].

```{r garchx-models}
stats <-
  map(
    tables,
    function(table_name) {
      models <- c_read_rds(
        c_paste_tight(
          "garchx_", table_name
        )
      )

      pb <- c_progress_setup(paste("Checking", table_name), length(models))

      calculated <-
        models %>%
        map(possibly(function(model) {
          pb$tick()

          (infocriteria(model) * length(fitted(model))) %>%
            enframe() %>%
            slice(1:2) %>%
            deframe() %>%
            array_branch() %>%
            setNames(c("aic", "bic"))
        }, otherwise = list(aic = NA, bic = NA))) %>%
        list_transpose()

      tibble(
        series = tables_full_names[table_name],
        model = models_for_testing_names,
        aic = calculated$aic,
        bic = calculated$bic
      )
    }
  )

best <-
  stats %>%
  map(~ .x %>%
    mutate(id = seq_len(nrow(.x))) %>%
    arrange(aic) %>%
    head(1) %>%
    pull(id)
  ) %>%
  setNames(tables)

seq_along(stats) %>%
  map(function(stat_id) {
    stats[[stat_id]][
      best[[stat_id]],
    ]
  }) %>%
  bind_rows() %>%
  c_kable(
    "Najlepsze wartości kryteriów informacyjnych dla modeli GARCH-X
    dopasowanych do stóp zwrotu badanych spółek",
    col.names = c(
      "Szereg czasowy",
      "Model",
      "AIC",
      "BIC"
    ),
    linesep = ""
  ) %>%
  add_header_above(
    c(" " = 2, "Kryterium informacyjne" = 2)
  )
```

```{r garchx-re-fit}
pb <- c_progress_setup(paste("Refitting"), length(tables))
garchx <-
  map(
    tables,
    function(table_name) {
      pb$tick()

      ugarchspec(
        variance.model = list(
          model = models_for_testing[best[[table_name]], ]$volatility,
          garchOrder = c(1, 1),
          external.regressors = as.matrix(garchx_externals[[table_name]])
        ),
        mean.model = list(
          armaOrder = arma[[table_name]]$arma[1:2],
          include.mean = FALSE
        ),
        distribution.model = models_for_testing[best[[table_name]], ]$distribution
      ) %>%
        ugarchfit(returns_split[[table_name]] %>% na.omit())
    }
  ) %>%
    setNames(tables)
```

```{r garchx-post-optimizing}
table <- map(
  tables,
  function(table_name) {
    garchx[[table_name]] %>%
      coef %>%
      enframe("param", "Estimate") %>%
      mutate(
        series = tables_full_names[table_name],
        model = models_for_testing_names[best[[table_name]]]
      )
  }
) %>%
  bind_rows() %>%
  pivot_wider(
    names_from = param,
    values_from = Estimate
  ) %>%
  select(-starts_with(c("ar", "ma")))

table %>%
  c_kable(
    "Parametry modeli GARCH-X dopasowanych do stóp zwrotu badanych spółek",
    col.names = model_params[colnames(.)],
    digits = 2,
    linesep = ""
  ) %>%
  add_header_above(c(
    " " = 2,
    "Parametry modelu" = length(colnames(table)[colnames(table) %in% c(
      "omega", "alpha1", "beta1", "gamma1", "vxreg1", "vxreg2"
    )]),
    "Par. rozkł. inn." = length(colnames(table)[colnames(table) %in% c(
      "skew", "shape"
    )])
  ))
```

Tabela \@ref(tab:garchx-models) przedstawia modele GARCH-X dopasowane do danych badanych spółek, wybrane na&nbsp;podstawie kryterium informacyjnego Akaike.
Można zauważyć, że każdy z modeli uwzględnia asymetrię modelu, ponieważ wyłonione zostały modele EGARCH oraz GJR.
Rozkłady innowacji dobrane do każdego z tych modeli stanowią zamienniki rozkładu normalnego, przy czym najczęściej w rankingu pojawia się rozkład GED.
Tabela \@ref(tab:garchx-post-optimizing) zawiera z kolei wartości parametrów wyestymowane dla tych modeli.

### Predykcje

```{r predictions-setup}
n_behind <- 10 * forecast_horizon
```

```{r predictions, fig.cap="Zestawienie prognoz wariancji warunkowej dla rozważanych modeli", fig.height=8}
sigmas <-
  list(
    markov = markov %>%
      map(function(model) {
        model %>%
          Volatility() %>%
          as.numeric()
      }),
    garchx = garchx %>%
      map(function(model) {
        model %>%
          sigma() %>%
          as.numeric()
      })
  )

past_vol <- sigmas %>%
  map(~ map(.x, function(x) tail(x, n_behind)))

pred_vol <-
  list(
    markov = markov %>%
      map(~
        predict(.x, nahead = forecast_horizon)$vol %>%
        tibble(pred_val = .) %>%
        pull(pred_val) %>%
        as.numeric()
      ),
    garchx = garchx %>%
      map(~ .x %>%
        ugarchforecast(n.ahead = forecast_horizon) %>%
        sigma() %>%
        tibble(pred_val = as.numeric(.)) %>%
        pull(pred_val) %>%
        head(forecast_horizon)
      )
  )

c_pred_plot(pred_vol, past_vol)
```

Posiadając odpowiednie modele dopasowane do danych, możliwym jest wykonanie prognoz ex-post i porównanie ich z rzeczywistym kształtowaniem się wartości odpowiadających indeksów giełdowych.
Przebieg prognozowanych szeregów czasowych ilustruje Rysunek \@ref(fig:predictions).
Zilustrowano na&nbsp;nim wariancję warunkową wynikającą z modelu dla ostatnich `r n_behind` dni estymacji,
a także prognozy, jakie zostały wykonane na&nbsp;`r forecast_horizon` dni naprzód.

## Ocena jakości dopasowania

Do oceny jakości dopasowania modeli do danych można posłużyć się znanymi w zagadnieniach ekonometrycznych miarami błędu dopasowania opisanymi w Równaniu \@ref(eq:errors): *RMSE* oraz *MAPE*.
Korzystają one z różnicy pomiędzy estymowaną a rzeczywistą wartością modelowanej wielkości (w&nbsp;tym przypadku zmienności) do pokazania bliskości wartości teoretycznych do empirycznych.

Przeszkodą w obliczeniu błędów dopasowania dla modeli zmienności warunkowej jest właśnie sama zmienność, która jest nieobserwowalna bezpośrednio.
Można ją jednak szacować i w tym celu wykorzystana została idea **zmienności zrealizowanej** z uwzględnieniem zwrotów nocnych oraz&nbsp;mikrostruktury rynku [@garmanZmiennosc].
Wówczas za pomocą cen otwarcia, maksymalnej, minimalnej i zamknięcia danego instrumentu finansowego w momencie $t$ (ozn. kolejno: $O_t, H_t, L_t, C_t$)
możliwym jest zdefiniowanie zmienności zrealizowanej na&nbsp;moment $t$ za pomocą wzoru:

\begin{equation} (\#eq:realized-volatility)
\hat{\sigma}^2_t =
  0.12 \frac{\nz{O_t - C_{t-1}}^2}{f}
  + 0.88 \frac{
    \frac{1}{2} \nz{H_t - L_t}^2
    - (2 \ln 2 - 1) \nz{C_t - O_t}^2
  }{1 - f}
\end{equation}

gdzie przez $f \in [0, 1]$ oznacza się część dnia, w jakiej giełda nie przeprowadza transakcji
-- w przypadku GPW, na&nbsp;której sesja trwa od godziny 9:00 do 17:00 wartość ta wynosi $f = \frac{2}{3}$.

```{r prediction-volatilities, fig.height=8, fig.cap="Porównanie zmienności zrealizowanej danych badanych spółek oraz zmienności estymowanej przez badane modele"}
volatilities <- map(
  tables,
  function(table_name) {
    data_split[[table_name]] %>%
      mutate(
        realized_volatility = sqrt(
          0.12 / stock_closed_per_day * (Otwarcie - lag(Zamkniecie))^2
          + 0.88 / (1 - stock_closed_per_day) * (
            0.5 * (Najwyzszy - Najnizszy)^2
            - (2 * log(2) - 1) * (Zamkniecie - Otwarcie)^2
          )
        )
      ) %>%
      select(Data, realized_volatility) %>%
      mutate(
        volatility_markov = c(NA, sigmas$markov[[table_name]]),
        volatility_garchx = c(NA, sigmas$garchx[[table_name]])
      )
  }) %>%
    setNames(tables)

volatilities %>%
  bind_rows(.id = "index") %>%
  pivot_longer(
    contains("volatility"),
    names_to = "type",
    names_transform = ~case_match(
      .x,
      "realized_volatility" ~ "zrealizowana",
      "volatility_markov" ~ "wg m. Markowa",
      "volatility_garchx" ~ "wg m. GARCH-X",
    )
  ) %>%
  mutate(
    index = tables_full_names[index]
  ) %>%
  ggplot(aes(Data, value, color = type)) +
    geom_line() +
    labs(
      x = "Data",
      y = "Odchylenie standardowe"
    ) +
    scale_color_discrete(name = "Zmienność") +
    scale_x_date(
      date_breaks = "4 months",
      date_minor_breaks = "1 month",
      date_labels = "%m.%y"
    ) +
    theme(
      panel.grid.major.x = element_line(color = "gray", size = 0.25),
      legend.position = "top"
    ) +
    facet_wrap(~index, ncol = 1, scales = "free")
```

```{r prediction-errors}
volatilities %>%
  bind_rows(.id = "index") %>%
  pivot_longer(
    starts_with("volatility"),
    names_to = "model",
    names_transform = ~str_replace(.x, "volatility_([a-z]*)", "\\1"),
    values_to = "conditional_volatility"
  ) %>%
  group_by(index, model) %>%
  na.omit() %>%
  summarise(
    rmse = sqrt(mean(
      (conditional_volatility - realized_volatility)^2
    )),
    mape = 100 * mean(
      na_if(
        abs(conditional_volatility - realized_volatility) / realized_volatility,
        Inf
      ),
      na.rm = TRUE
    )
  ) %>%
  pivot_wider(
    names_from = model,
    values_from = rmse:mape
  ) %>%
  relocate(contains("garchx"), .after = mape_markov) %>%
  mutate(
    index = tables_full_names[index]
  ) %>%
  c_kable(
    "Zestawienie błędów dopasowania modeli zmienności warunkowej do danych",
    col.names = c(
      "Szer. czas.",
      rep(c("RMSE", "MAPE"), 2)
    ),
    linesep = ""
  ) %>%
  add_header_above(c(
    " " = 1,
    "model Markowa" = 2,
    "GARCH-X" = 2
  ))
```

Wartości zmienności zrealizowanej zostały wykorzystane w miejsce zmienności rzeczywistej w celu obliczenia błędów dopasowania.
Wyniki tych obliczeń przedstawia Tabela \@ref(tab:prediction-errors).
Wartości osiągnięte przez wskaźniki nie różnią się w dużym stopniu pomiędzy modelami.
W wybranych przypadkach można zauważyć bardzo wysokie wskazania błędów dla modelu GARCH-X,
głównie z&nbsp;uwagi na&nbsp;niski poziom zmienności zrealizowanej.
